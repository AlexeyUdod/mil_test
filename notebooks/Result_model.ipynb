{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbYv--7JNhmh"
   },
   "source": [
    "# Задача"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DiPAzVQmNhmj"
   },
   "source": [
    "**Требуется:** предложить модель, сегментирующую человека на фотографии (без использования предобученных и готовых моделей).  \n",
    "  \n",
    "**Вход:** фотография 320x240x3.  \n",
    "**Выход:** маска человека 320x240.  \n",
    "**Метрика:** [Dice coefficient](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient).  \n",
    "  \n",
    "\n",
    "Данные представляют из себя набор фотографий человека и маски, определяющей положение человека на фотографии.  \n",
    "Доступные данные разделены на несколько папок:  \n",
    "- `train` содержит фотографии 320x240x3;\n",
    "- `train_mask` содержит маски для фотографий из `train` 320x240;\n",
    "- `valid` содержит фотографии 320x240x3;\n",
    "- `valid_mask` содержит маски для фотографий из `valid` 320x240;\n",
    "- `test` содержит фотографии 320x240x3.  \n",
    "  \n",
    "Для лучшей модели требуется создать 2 файла, которые необходимы для валидации решения:\n",
    "- сохраненные маски для картинок из valid в формате pred_valid_template.csv (в архиве с `data`);\n",
    "- html страницу с предсказанием модели для всех картинок из test и папку с используемыми картинками в этой html странице для её просмотра."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Решение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWlB8Kuwwi9R"
   },
   "source": [
    "Так как в задании прописаны требования по созданию сети с нуля, то нет возможности использовать любые предобученные модели или архитектуры из коробки, (классификационные предобученные сети сильно бы помогли). \n",
    "\n",
    "Для функции потерь dice loss подходит слабо, так как она слишком сильно меняется в зависимости от пересечения и не дает хорошей дополнительной информации для обучения. Изначально была выбрана попиксельный L1 loss c дальнейшим добавлением коэфициента зоны перехода границы маски, чтобы сеть больше внимания уделяла области между человеком и фоном. Коэфициент зоны меняется в зависимости от степени обучения.\n",
    "\n",
    "За baseline была взята собранная с нуля архитектура Unet так как у нас бинарная классификация, которая достаточно похожа на медицинские данные предназначенные для Unet.\n",
    "\n",
    "Слишком глубокий Unet плохо обучается и часто вылетает из-за недостатка видеопамяти. Выбрана глубина 8, 16, 32, 64, 128 слоем на каждом уровне соответственно\n",
    "\n",
    "В качестве оптимизатора был взят Adam со скоростью обучения 0.0001. При более высокой скорости обучения сеть не успевает найти нужные признаки и сваливается в наиболее вероятные положения масок во всем датасете, то есть начинает предсказывать прямоугольник по центру для любой фото. \n",
    "\n",
    "Добавление dropout 2D на центральный боттлнек не дало улучшения.\n",
    "То же самое относится к BatchNorm, который приводит к нестабильности обучения (хотя в теории должно быть наоборот)\n",
    "\n",
    "Из аугментации были выбранны только базовые измения цветов и небольшие трансформации из пакета torchvision, так как любая, чуть более сильная аугментация, приводит сеть к осциляции между полной маской и полностью пустой маской.\n",
    "\n",
    "Гипотеза о начальном обучении сети на фото с вырезанным человеком и дальнейшим плавным проявлением фона в зависимости от метрики не привела к улучшению, так как сеть выучивала только границу между фоном и маской и отказывалась искать другие признаки.\n",
    "\n",
    "Дальше в архитектуру были добавлены 2 residual connections между входным и  классифицирующим выходными слоем.\n",
    "Финальный слой был расширен. Так же был  добавлен еще один классификатор (тоже сверка 1х1, для повышения вариативности при увеличении количества слов и преодоление XOR-подобных проблем). Это добавило стабильности обучения.\n",
    "\n",
    "Использованние маскирующих сверток  на входе сети позволяет прибавить до 1% dice, но приводит к сильной нестабильности  и затыкам обучения. В финальной модели они не используются. \n",
    "\n",
    "Добавлены Squeeze-and-Excitation блоки, так как в бинарной сегментации хорошо себя показывают использование предобученных архитектур на основе SE слоев (например SE-ResNeXt-50). Это улучшило dice на 2%.\n",
    "\n",
    "Для предобработки входных фото используются кастомные сверточные слои с фиксированными весами распределения Гаусса имеющие различный сдвиг по амлитуде. Это позволяет выделять грани переходов, обратные грани, и общий цвет зоны для каждого канала индивидуально. Применяя свертки различного размера можно получить подобие спектральных каналов. Были выбранные 3 размера сверток размерами 5, 9 и 21 пиксель, что все вместе дает на вход сети 27 канальное изображение. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 380,
     "status": "ok",
     "timestamp": 1627207426172,
     "user": {
      "displayName": "Alexey Udod",
      "photoUrl": "",
      "userId": "09887141178389253930"
     },
     "user_tz": -180
    },
    "id": "_SgiRH23NnN6",
    "outputId": "446d12b2-345b-49b6-8e7c-3a70572f99ef"
   },
   "outputs": [],
   "source": [
    "import platform\n",
    "if platform.system() == 'Linux':\n",
    "    path0 = '/content/drive/My Drive/Colab Notebooks/MFTI/test/mipt_test_git'\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    import sys\n",
    "    sys.path.insert(1, path0)\n",
    "else:\n",
    "    path0 = '/MFTI/test/mipt_test_git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_JgsC72xNhmT"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from glob import glob   \n",
    "\n",
    "from lib import *\n",
    "\n",
    "import tqdm\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import torch as tr\n",
    "import torch.fft as trf\n",
    "import torch.distributions as trd\n",
    "import torchvision.transforms as tvt\n",
    "t = tr.tensor\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 116,
     "referenced_widgets": [
      "5335b1b1dd92421bb0c0d5c253d159dd",
      "e30ec208050146afb110b6c960a50d1c",
      "d40156fc98e6440e990d6fbcb1f6c01e",
      "da1307cdbdba46a1b88e8aadae0ba7ed",
      "e2c401fee0fa433d85578bd9694c9733",
      "0ca328ccad094c9bb80611373ff49bfc",
      "19e60de5a7c94ef8bb7a17bd69a871bf",
      "8b5e34bb52114d688816b90c3270f22f",
      "8c54da2599b84f79afad86412c3d3fd4",
      "d27b5d5cfdfd4692bc5a9e754b4de813",
      "d41cb2b44eb14fffa1be82a1e5d6ca80",
      "f5ac3d6afe094fb8a417e2f396517359",
      "07a174144c11429d95cd976e5226f587",
      "9346e7b0eff8473fa88a00ea97d6288e",
      "ed1f900ec99645a6add27986bdf60dab",
      "a9b822fb488446a19e43ffab8262e0ee"
     ]
    },
    "executionInfo": {
     "elapsed": 8401,
     "status": "ok",
     "timestamp": 1627207435748,
     "user": {
      "displayName": "Alexey Udod",
      "photoUrl": "",
      "userId": "09887141178389253930"
     },
     "user_tz": -180
    },
    "id": "lPGp6RIRARXL",
    "outputId": "f7703bca-8d1e-425c-9eac-2738a846db25"
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "to_tensor_ = tvt.ToTensor()\n",
    "\n",
    "for i, data_type in enumerate(['train', 'valid']):\n",
    "    images = os.listdir(path0 + f'/data/{data_type}')\n",
    "    r = []\n",
    "    for im in tqdm.auto.tqdm(images):\n",
    "        i = (to_tensor_(Image.open(f'{path0}/data/{data_type}/{im}')), \n",
    "            to_tensor_(Image.open(f\"{path0}/data/{data_type}_mask/{im.split('.')[0]}.png\")))\n",
    "        r.append(i)\n",
    "    res.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JX7hwWXzNRXv"
   },
   "outputs": [],
   "source": [
    "class SE(tr.nn.Module):\n",
    "    \"\"\"Squeeze-and-Excitation block\n",
    "    https://arxiv.org/pdf/1709.01507.pdf\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ch, r=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = tr.nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = tr.nn.Sequential(\n",
    "            tr.nn.Linear(ch, ch // r, bias=False),\n",
    "            tr.nn.ReLU(inplace=True),\n",
    "            tr.nn.Linear(ch // r, ch, bias=False),\n",
    "            tr.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DIhQgdEd30V0"
   },
   "outputs": [],
   "source": [
    "class Unet(tr.nn.Module):\n",
    "    \n",
    "    def __init__(self, n=[1, 32, 64, 128, 256, 512], padd = 'same', err = False):\n",
    "        super().__init__()\n",
    "        self.err = err\n",
    "        self.n = n\n",
    "        \n",
    "        # self.mask11 = tr.nn.Conv2d(9, n[1], 3, padding = padd)\n",
    "        # self.mask12 = tr.nn.Conv2d(n[1], n[1], 3, padding = padd)\n",
    "        # self.mask13 = tr.nn.Conv2d(n[1], n[1], 3, padding = padd)\n",
    "\n",
    "        self.conv11 = tr.nn.Conv2d(27, n[1], 3, padding = padd)\n",
    "        self.conv12 = tr.nn.Conv2d(n[1], n[1], 3, padding = padd)\n",
    "        self.se13 = SE(n[1])\n",
    "\n",
    "        # self.mask20 = tr.nn.Conv2d(n[1], n[2], 3, padding = padd)\n",
    "        self.conv21 = tr.nn.Conv2d(n[1], n[2], 3, padding = padd)\n",
    "        self.conv22 = tr.nn.Conv2d(n[2], n[2], 3, padding = padd)\n",
    "        self.se23 = SE(n[2])\n",
    "\n",
    "        self.conv31 = tr.nn.Conv2d(n[2], n[3], 3, padding = padd)\n",
    "        self.conv32 = tr.nn.Conv2d(n[3], n[3], 3, padding = padd)\n",
    "        self.se33 = SE(n[3])\n",
    "\n",
    "        self.conv41 = tr.nn.Conv2d(n[3], n[4], 3, padding = padd)\n",
    "        self.conv42 = tr.nn.Conv2d(n[4], n[4], 3, padding = padd)\n",
    "        self.se43 = SE(n[4])\n",
    "\n",
    "        self.conv51 = tr.nn.Conv2d(n[4], n[5], 3, padding = padd)\n",
    "        self.conv52 = tr.nn.Conv2d(n[5], n[5], 3, padding = padd)\n",
    "        self.se53 = SE(n[5])\n",
    "\n",
    "        self.deconv60 = tr.nn.ConvTranspose2d(n[5], n[4], 3, padding=padd, output_padding=padd,   stride=2)\n",
    "        self.conv61 = tr.nn.Conv2d(n[5], n[4], 3, padding = padd)\n",
    "        self.conv62 = tr.nn.Conv2d(n[4], n[4], 3, padding = padd)\n",
    "        self.se63 = SE(n[4])\n",
    "\n",
    "        self.deconv70 = tr.nn.ConvTranspose2d(n[4], n[3], 3, padding=padd, output_padding=padd, stride=2)\n",
    "        self.conv71 = tr.nn.Conv2d(n[4], n[3], 3, padding = padd)\n",
    "        self.conv72 = tr.nn.Conv2d(n[3], n[3], 3, padding = padd)\n",
    "        self.se73 = SE(n[3])\n",
    "\n",
    "        self.deconv80 = tr.nn.ConvTranspose2d(n[3], n[2], 3, padding=padd, output_padding=padd, stride=2)\n",
    "        self.conv81 = tr.nn.Conv2d(n[3], n[2], 3, padding = padd)\n",
    "        self.conv82 = tr.nn.Conv2d(n[2], n[2], 3, padding = padd)\n",
    "        self.se83 = SE(n[2])\n",
    "\n",
    "        self.deconv90 = tr.nn.ConvTranspose2d(n[2], n[1], 3, padding=padd, output_padding=padd, stride=2)\n",
    "        self.conv91 = tr.nn.Conv2d(n[2]+27, n[1], 3, padding = padd)\n",
    "        self.conv92 = tr.nn.Conv2d(n[1], n[1], 3, padding = padd)\n",
    "        self.se93 = SE(n[1])\n",
    "\n",
    "        self.conv100 = tr.nn.Conv2d(n[1]+27, n[-1], 1)\n",
    "        self.se103 = SE(n[-1])\n",
    "        if self.err:\n",
    "            self.conv110 = tr.nn.Conv2d(n[-1], 2, 1)\n",
    "        else:\n",
    "            self.conv110 = tr.nn.Conv2d(n[-1], 1, 1)\n",
    "\n",
    "\n",
    "        self.maxpool = tr.nn.MaxPool2d(2)\n",
    "        self.bn = tr.nn.BatchNorm2d(n[5])\n",
    "        self.m0 = 0\n",
    "\n",
    "    def forward(self, input, dropout_koef):\n",
    "\n",
    "        dropout  = tr.nn.Dropout2d(dropout_koef)\n",
    "\n",
    "        r = tr.nn.ReLU()\n",
    "        sig = tr.nn.Sigmoid()\n",
    "\n",
    "        x = input\n",
    "\n",
    "        # m = r(self.mask11(x))\n",
    "        # m = (self.mask12(m))\n",
    "        # m = r(self.mask13(m))\n",
    "        if str(type(self.m0)) in \"<class 'torch.Tensor'>\" :\n",
    "          x *= self.m0.mean(dim=0).unsqueeze(0)\n",
    "\n",
    "        x = r(self.conv11(x))\n",
    "        # x = r(m * x)\n",
    "        x = r(self.conv12(x))\n",
    "        x = self.se13(x)\n",
    "        x1 = x\n",
    "        x = self.maxpool(x1)\n",
    "\n",
    "        # m = r(self.mask20(x))\n",
    "        x = r(self.conv21(x))\n",
    "        # x *= m\n",
    "        x = r(self.conv22(x))\n",
    "        x = self.se23(x)\n",
    "        x2 = x\n",
    "        x = self.maxpool(x2)\n",
    "        \n",
    "        x = r(self.conv31(x))\n",
    "        x = r(self.conv32(x))\n",
    "        x = self.se33(x)\n",
    "        x3 = x\n",
    "        x = self.maxpool(x3)\n",
    "       \n",
    "        x = r(self.conv41(x))\n",
    "        x = r(self.conv42(x))\n",
    "        x = self.se43(x)\n",
    "        x4 = x\n",
    "        x = self.maxpool(x4)\n",
    "       \n",
    "        x = r(self.conv51(x))\n",
    "        x = r(self.conv52(x))\n",
    "        x = self.se53(x)\n",
    "        x5 = x\n",
    "        \n",
    "        x = tr.cat([self.deconv60(x5), x4], dim=1)\n",
    "        x = r(self.conv61(x))\n",
    "        x6 = r(self.conv62(x))\n",
    "        x = self.se63(x)\n",
    "\n",
    "        x = tr.cat([self.deconv70(x6), x3], dim=1)\n",
    "        x = r(self.conv71(x))\n",
    "        x7 = r(self.conv72(x))\n",
    "        x = self.se73(x)\n",
    "\n",
    "        x = tr.cat([self.deconv80(x7), x2], dim=1)\n",
    "        x = r(self.conv81(x))\n",
    "        x8 = r(self.conv82(x))\n",
    "        x = self.se83(x)\n",
    "\n",
    "        x = tr.cat([self.deconv90(x8), x1, input], dim=1)\n",
    "        x = r(self.conv91(x))\n",
    "        x9 = r(self.conv92(x))\n",
    "        x = self.se93(x)\n",
    "\n",
    "        x = x9\n",
    "        x = tr.cat([x, input], dim=1)\n",
    "\n",
    "        x = sig(self.conv100(x))\n",
    "\n",
    "        # err = x[:,0:1]\n",
    "\n",
    "        x = self.se103(x)\n",
    "\n",
    "        err = x[:,0:1]\n",
    "        m = x[:,1:2]\n",
    "        x1 = x[:,1:2]\n",
    "        x = x * m\n",
    "\n",
    "        size = int(x.shape[1] / 2)\n",
    "        m = x[:,:size]\n",
    "        x1 = x[:,size:]\n",
    "        x = tr.cat([x1, x1 * m], dim = 1)\n",
    "\n",
    "        x = sig(self.conv110(x))\n",
    "\n",
    "        pred = x[:, 0].unsqueeze(1) \n",
    "        # err = x[:, 1].unsqueeze(1)\n",
    "\n",
    "        if self.err:\n",
    "            return pred, err\n",
    "        else:\n",
    "            return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1627207436126,
     "user": {
      "displayName": "Alexey Udod",
      "photoUrl": "",
      "userId": "09887141178389253930"
     },
     "user_tz": -180
    },
    "id": "aY8iyVmOHQ5o",
    "outputId": "14713bdb-de5e-4977-bbae-a5ec2c897d19"
   },
   "outputs": [],
   "source": [
    "dev = tr.device('cuda:0' if tr.cuda.is_available() else 'cpu')\n",
    "print(f\"work on {(tr.cuda.get_device_name() if dev.type == 'cuda' else 'cpu')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-09S8BiHDan"
   },
   "outputs": [],
   "source": [
    "def dice(y, pred):\n",
    "    \"\"\"Dice loss\n",
    "    \n",
    "    Args:\n",
    "        y (tensor): true mask [batch_size, 1, H, W]\n",
    "        pred (tensor): pred mask [batch_size, 1, H, W]\n",
    "    Returns:\n",
    "        dice_loss (tensor): [1]\n",
    "    \"\"\"\n",
    "\n",
    "    true = (y >= 0.5).to(dev)\n",
    "    pred = pred >= 0.5\n",
    "    intersection = (true * pred).sum()\n",
    "    im_sum = true.sum() + pred.sum()\n",
    "    return 2.0 * intersection / (im_sum + 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vf_tQiGK2K6d"
   },
   "outputs": [],
   "source": [
    "def create_conv_kernels(types_params, X_shape):\n",
    "    \"\"\"Make tensor of conv kernels as 2D normal distributions \n",
    "    with 5 parameters (E_x, E_y, D_x, D_y, bias) \n",
    "    from types_params tensor.\n",
    "\n",
    "    Args:\n",
    "        types_params (tensor): gaussian params [num_kernels, 5].\n",
    "        X_shape (tensor.shape): data shapes for kernel size.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of Gaussian kernels with shapes\n",
    "        [num_kernels, data.shape[-1], data.shape[-2]] \n",
    "    \"\"\"\n",
    "        \n",
    "    im_center = (t(X_shape[-2:]) / 2).int()\n",
    "    kernels = []\n",
    "    for params in types_params:\n",
    "        E_xy = params[:2] + im_center\n",
    "        D_xy = tr.diag(params[2:4])\n",
    "        grid_xy = tr.cartesian_prod(\n",
    "            tr.arange(X_shape[-2]), \n",
    "            tr.arange(X_shape[-1])\n",
    "            ).reshape(*X_shape[-2:], 2)\n",
    "        bias = params[4]\n",
    "        # print(D_xy)\n",
    "        kernel = trd.MultivariateNormal(\n",
    "            E_xy, D_xy\n",
    "            ).log_prob(grid_xy).exp()#.to(dev)\n",
    "        kernel -= kernel.mean() * bias\n",
    "        kernel /= kernel.max()\n",
    "        kernels.append(kernel.unsqueeze(0))\n",
    "    return tr.cat(kernels, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BSZAD0np0YrO"
   },
   "outputs": [],
   "source": [
    "def augments(x, y, aug_koef=0.5):\n",
    "    \"\"\"Make augmentations:\n",
    "    RandomPerspective, RandomAffine, \n",
    "    RandomInvert, ColorJitter and color channels permutations\n",
    "\n",
    "    Args:\n",
    "        X (tensor): images [batch_size, channels, H, W]\n",
    "        y (tensor): true mask [batch_size, 1, H, W]\n",
    "        aug_koef (int): aug probability\n",
    "\n",
    "    Returns:\n",
    "        X (tensor): augmented images [batch_size, channels, H, W]\n",
    "        y (tensor): augmented true mask [batch_size, 1, H, W]\n",
    "    \"\"\"\n",
    "    aug_koef = aug_koef % 1\n",
    "    y1 = y  \n",
    "    # xy = tr.cat([x, y], dim=1).to(dev)\n",
    "    # xy = tvt.RandomPerspective(distortion_scale=0.3, p=aug_koef,  fill=0)(xy)\n",
    "    # xy = tvt.RandomAffine(30 * aug_koef, translate = (aug_koef, aug_koef))(xy)\n",
    "    # y1 = xy[:, -1].unsqueeze(1)\n",
    "    # x = xy[:, :-1]\n",
    "    \n",
    "    # x = tvt.transforms.RandomInvert(p=aug_koef)(x)\n",
    "    # x = x[:, tr.randperm(3),:,:]\n",
    "    # diap = ((1-aug_koef), 1*(1-aug_koef))\n",
    "  \n",
    "    x = tvt.ColorJitter(brightness=(0.5, 2), contrast=(0.5, 2) , \n",
    "                        saturation=(0.5, 2), hue=(-0.2, 0.2))(x)\n",
    "    return x, y1\n",
    "    \n",
    "def blend_xy(X, y, koef):\n",
    "    \"\"\"Add some y information to X, for quick start \n",
    "\n",
    "    Args:\n",
    "        X (tensor): images [batch_size, channels, H, W]\n",
    "        y (tensor): true mask [batch_size, 1, H, W]\n",
    "        koef (int): mix koef (1 - clear X, 0 - clear y)\n",
    "\n",
    "    Returns:\n",
    "        X (tensor): images [batch_size, channels, H, W]\n",
    "    \"\"\"\n",
    "\n",
    "    return X * koef +  y * (1 - koef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W4jbNIw_UU5d"
   },
   "outputs": [],
   "source": [
    "def make_edge_conv(in_ch, out_ch, kernel_size):\n",
    "    \"\"\"Make conv layer for image preprocessing \n",
    "    from 3 RGB channels to 9 spectral channels\n",
    "\n",
    "    Args:\n",
    "        in_ch (int): input image channels \n",
    "        out_ch (int): output channels (default = 9)\n",
    "        kernel_size (int): conv kernel size\n",
    "\n",
    "    Returns:\n",
    "        edges (torch.nn.Conv2d): conv layer\n",
    "    \"\"\"\n",
    "\n",
    "    pad = int((kernel_size - 1)/2)\n",
    "    out_ch = 9\n",
    "    edges = tr.nn.Conv2d(in_ch, out_ch, kernel_size, padding=pad).to(dev)\n",
    "    types_params = t([[0, 0, pad, pad, 1]]).float()\n",
    "    edge = create_conv_kernels(types_params, edges.weight.shape)  #.shape\n",
    "    edges.weight.data = tr.zeros_like(edges.weight.data)\n",
    "\n",
    "    for in_ch in range(edges.weight.data.shape[1]):\n",
    "        edges.weight.data[in_ch*3+0, in_ch] = edge[0]\n",
    "        edges.weight.data[in_ch*3+1, in_ch] = -edge[0]\n",
    "        edges.weight.data[in_ch*3+2, in_ch] = edge[0] - edge[0].min()\n",
    "\n",
    "    edges.weight.data /= edges.weight.data.max()\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7AtcHyj0NPQ"
   },
   "outputs": [],
   "source": [
    "def show_imgs(X, y, pred, err, title):\n",
    "    \"\"\"Show random X image with true and pred mask\n",
    "\n",
    "    Args:\n",
    "        X (tensor)[batch_size, channels, H, W]: images\n",
    "        y (tensor): true mask [batch_size, 1, H, W]\n",
    "        pred (tensor): pred mask [batch_size, 1, H, W]\n",
    "        title (str): image title to show\n",
    "    \"\"\"\n",
    "\n",
    "    X = X.cpu().detach() \n",
    "    y = y.cpu().detach() \n",
    "    err = err.cpu().detach() \n",
    "    pred = (pred / pred.max() * 255).int().cpu().detach()\n",
    "    i = tr.randint(X.shape[0], (1,)).item()\n",
    "  \n",
    "    plt.subplot(151)\n",
    "    plt.title(title)\n",
    "    plt.imshow(X[i, :3].permute(1,2,0))\n",
    "\n",
    "    plt.subplot(152)\n",
    "    plt.imshow(X[i, 3:6].permute(1,2,0))\n",
    "\n",
    "    if 'torch' in err.type():\n",
    "        plt.subplot(153)\n",
    "        plt.imshow(err[i, 0])\n",
    "    else:\n",
    "        plt.subplot(153)\n",
    "        plt.imshow(X[i, 6:9].permute(1,2,0))\n",
    "\n",
    "    plt.subplot(154)\n",
    "    plt.imshow(pred[i, 0])\n",
    "\n",
    "    plt.subplot(155)\n",
    "    plt.imshow(y[i, 0])\n",
    "\n",
    "    plt.gcf().set_size_inches(20, 4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 325,
     "status": "ok",
     "timestamp": 1627211049087,
     "user": {
      "displayName": "Alexey Udod",
      "photoUrl": "",
      "userId": "09887141178389253930"
     },
     "user_tz": -180
    },
    "id": "M_alprrWJoRY",
    "outputId": "2043d955-129b-49ea-8332-4ea29e4044ea"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xxI4qnGGs_DM"
   },
   "outputs": [],
   "source": [
    "def crit(y, pred, y_edges, edge_loss_koef, error = 0):\n",
    "    \"\"\"Loss func for images with importance koef of mask edges\n",
    "\n",
    "     Args:\n",
    "        y (tensor): true mask [batch_size, 1, H, W]\n",
    "        pred (tensor): pred mask [batch_size, 1, H, W]\n",
    "        y_edges (tensor): mask edges [batch_size, 1, H, W]\n",
    "        edge_loss_koef (int): mask edges importance koef \n",
    "\n",
    "    Returns:\n",
    "        loss (tensor): [1]\n",
    "    \"\"\"\n",
    "    \n",
    "    # (err + y - pred).abs().sum()\n",
    "    edge_zone = (y_edges*(1 - edge_loss_koef) + edge_loss_koef)\n",
    "    loss = ((edge_zone * (y - pred)).abs().sum() + (error - y + pred).abs().sum()) / tr.tensor(y.shape).prod() / 2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lq5GJW_uat4A"
   },
   "outputs": [],
   "source": [
    "edges_x1 = make_edge_conv(3, 9, 5)\n",
    "edges_x2 = make_edge_conv(3, 9, 9)\n",
    "edges_x3 = make_edge_conv(3, 9, 21)\n",
    "\n",
    "edges = tr.nn.Conv2d(1, 1, 21, padding=10).to(dev)\n",
    "types_params = t([[0, 0, 10, 10, 1]]).float()\n",
    "edges.weight.data[0] = create_conv_kernels(types_params, edges.weight.shape)\n",
    "\n",
    "d = {k:v for v, k in enumerate(['iter', 'loss_valid', 'loss_valid_best', \n",
    "                        'loss', 'loss_train_best', 'dice', \n",
    "                        'time', 'time_iter', 'memory'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U05LkuJZpHQC"
   },
   "outputs": [],
   "source": [
    "train_loader = tr.utils.data.DataLoader(res[0], batch_size = 20, shuffle = True)\n",
    "valid_loader = tr.utils.data.DataLoader(res[1], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1vfuNE0jzUBQrnUXaZPTqMagavrmdLxWu"
    },
    "id": "zCEutRhw30bL",
    "outputId": "5f80f6c9-6434-4a9f-8c65-fd163669e4cc"
   },
   "outputs": [],
   "source": [
    "models = {}\n",
    "n_iters = 100\n",
    "history = tr.zeros(1, n_iters, len(d))\n",
    "timer_prev = time.time()\n",
    "with_err = True\n",
    "err = 0\n",
    "loss_valid = 0\n",
    "for model_num, p in enumerate(range(3, 7)):\n",
    "\n",
    "    unet_params = [2**n for n in range(p, p + 5)]\n",
    "    \n",
    "    tr.cuda.empty_cache()\n",
    "    model = Unet([1] + unet_params, 1, with_err).to(dev)\n",
    "    # model_num = 2\n",
    "    name = str(model.__class__()).split('(')[0]\n",
    "    models[name] = {model_num:{'params':unet_params}}\n",
    "    print('\\n', name, unet_params)\n",
    "\n",
    "    optim = tr.optim.Adam(model.parameters(), 0.0001)\n",
    "\n",
    "    loss_train_best = 1e+10\n",
    "    loss_valid_best = 1e+10\n",
    "\n",
    "    blend_koef = 0\n",
    "    edge_loss_koef = 0.7\n",
    "    dropout_koef = 0.1\n",
    "    aug_koef = 0.2\n",
    "    history_model = tr.zeros(1, len(d), dtype=tr.float64)\n",
    "    model_start = time.time()\n",
    "\n",
    "\n",
    "    for iter in range(1, n_iters):\n",
    "        model.train()\n",
    "        for X, y in train_loader:\n",
    "            X, y = X.to(dev), y.to(dev)\n",
    "            # X = blend_xy(X, y, blend_koef)\n",
    "            \n",
    "            X, y = augments(X, y, aug_koef)  \n",
    "\n",
    "            # if background.shape != X.shape:\n",
    "            #     types_params = tr.randint(1, X.shape[-1], (t(X.shape[:2]).prod(), 5)).float()\n",
    "            #     background = create_conv_kernels(types_params, X.shape).reshape(X.shape).to(dev)\n",
    "            #     background /= background.max()\n",
    "            # X[X == 0] = background[X == 0]\n",
    "\n",
    "            X = tr.cat([edges_x1(X), edges_x2(X), edges_x3(X)], dim=-3)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            if with_err:\n",
    "                pred, err = model(X, dropout_koef)\n",
    "            else:\n",
    "                pred = model(X, dropout_koef)\n",
    "\n",
    "            y_edges = edges(y).abs()\n",
    "            loss = crit(y, pred, y_edges, edge_loss_koef, err)\n",
    "\n",
    "            edge_loss_koef = 1 - loss.detach()\n",
    "            aug_koef = loss.detach()\n",
    "            # dropout_koef = loss.detach() / 100\n",
    "            blend_koef = 1 - loss.detach() / 10\n",
    "\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            if loss < loss_train_best:\n",
    "                model_train_best = model\n",
    "                loss_train_best = loss   \n",
    "                dice_train = dice(y, pred).item()  \n",
    "                show_imgs(X, y,pred, err, f'Train at iter {iter}, train_loss {loss}, dice_train {dice_train}')\n",
    "            \n",
    "        model.eval()\n",
    "        for X, y in valid_loader:\n",
    "            X, y = X.to(dev), y.to(dev)\n",
    "            X = tr.cat([edges_x1(X), edges_x2(X), edges_x3(X)], dim=-3)\n",
    "            if with_err:\n",
    "                pred, err = model(X, dropout_koef)\n",
    "            else:\n",
    "                pred = model(X, dropout_koef)\n",
    "\n",
    "            y_edges = edges(y).abs()\n",
    "\n",
    "            loss_valid = crit(y, pred, y_edges, edge_loss_koef, err)\n",
    "            dice_valid = dice(y, pred).item()\n",
    "            \n",
    "            timer = time.time()\n",
    "            time_iter = timer - timer_prev\n",
    "            memory = round(tr.cuda.memory_allocated() * 1e-6)\n",
    "\n",
    "            if loss_valid < loss_valid_best:\n",
    "                model_valid_best = model\n",
    "                loss_valid_best = loss_valid.item()\n",
    "                models[name][model_num].update({\n",
    "                    'loss_valid_best':loss_valid_best,\n",
    "                    'dice_valid':dice_valid, \n",
    "                    'iter': iter, \n",
    "                    'time': timer - model_start, \n",
    "                    'model_valid_best':model_valid_best, })\n",
    "                \n",
    "                show_imgs(X, y, pred, err,  f'Valid at iter {iter}, val_loss {loss_valid}, dice_valid {dice_valid}')\n",
    "\n",
    "            h = tr.tensor([iter, loss_valid, loss_valid_best, \n",
    "                            loss.item(), loss_train_best, dice_valid, \n",
    "                            timer, time_iter, memory])\n",
    "            history_model = tr.cat([history_model, h.unsqueeze(0)])\n",
    "            print(f\"iter {h[d['iter']].int()} at {h[d['time_iter']].int()} sec, loss_valid {round(h[d['loss_valid']].item(), 2)}, dice {round(h[d['dice']].item(), 2)}, memory {h[d['memory']]} MB, loss {history_model[-1, d['loss']].item()}\")\n",
    "            timer_prev = timer\n",
    "            break\n",
    "    with open(path0 + '/data/models.pkl', 'wb') as fp:\n",
    "        pickle.dump(models, fp)\n",
    "    history = tr.cat([history, history_model.unsqueeze(0)])\n",
    "    pred = model_valid_best(X, 0)\n",
    "    show_imgs(X, y, pred, f'Valid at iter {iter}, val_loss {loss_valid}, dice {dice_valid}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 262,
     "status": "ok",
     "timestamp": 1627211113637,
     "user": {
      "displayName": "Alexey Udod",
      "photoUrl": "",
      "userId": "09887141178389253930"
     },
     "user_tz": -180
    },
    "id": "lET572zpVwUK",
    "outputId": "61fd5178-d92b-4b33-8c3b-013861c9b7fe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68XxLypQAkV_"
   },
   "outputs": [],
   "source": [
    "with open(path0 + '/data/models.pkl', 'rb') as fp:\n",
    "    models = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wuPrWCwdoTip"
   },
   "outputs": [],
   "source": [
    "models['Unet'][0].keys()\n",
    "best_model = models['Unet'][0]['model_valid_best']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "executionInfo": {
     "elapsed": 277,
     "status": "error",
     "timestamp": 1627227725970,
     "user": {
      "displayName": "Alexey Udod",
      "photoUrl": "",
      "userId": "09887141178389253930"
     },
     "user_tz": -180
    },
    "id": "CrkXfkAxEEbf",
    "outputId": "4d5a88c4-eeac-4051-fe42-0bc42c88f8d0"
   },
   "outputs": [],
   "source": [
    "valid_loader = tr.utils.data.DataLoader(res[1], 10)\n",
    "best_model.eval()\n",
    "for X, y in valid_loader:\n",
    "    X, y = X.to(dev), y.to(dev)\n",
    "    X = tr.cat([edges_x1(X), edges_x2(X), edges_x3(X)], dim=-3)\n",
    "    pred = best_model(X, 0)\n",
    "    dice_valid = dice(y, pred).item()\n",
    "    memory = round(tr.cuda.memory_allocated() * 1e-6)\n",
    "    show_imgs(X, y, pred, f'Valid with dice {dice_valid}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2adt8dTEYnkL"
   },
   "source": [
    "Сохрание результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 405,
     "status": "ok",
     "timestamp": 1627227711451,
     "user": {
      "displayName": "Alexey Udod",
      "photoUrl": "",
      "userId": "09887141178389253930"
     },
     "user_tz": -180
    },
    "id": "C3BMG8FgJ8R2"
   },
   "outputs": [],
   "source": [
    "def id2rle(id):\n",
    "    to_tensor_ = tvt.ToTensor()\n",
    "    im = to_tensor_(Image.open(f\"{path0}/data/valid/{id}.jpg\")).unsqueeze(0).to(dev)\n",
    "    X = tr.cat([edges_x1(im), edges_x2(im), edges_x3(im)], dim=-3)\n",
    "    pred = (best_model(X, 0) > 0.5) * 1\n",
    "    r = encode_rle(pred.cpu().detach())\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRmMjr_DY0hy"
   },
   "source": [
    "Записываем csv c rle масками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "error",
     "timestamp": 1627227678635,
     "user": {
      "displayName": "Alexey Udod",
      "photoUrl": "",
      "userId": "09887141178389253930"
     },
     "user_tz": -180
    },
    "id": "PQ449yW3OERU",
    "outputId": "cb97f51b-ceb2-41ac-fabf-f1a839f8ec5d"
   },
   "outputs": [],
   "source": [
    "pred_pd['rle_mask'] = pred_pd.id.apply(id2rle)\n",
    "pred_pd.to_csv(path0 + \"/data/pred_valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1627227679584,
     "user": {
      "displayName": "Alexey Udod",
      "photoUrl": "",
      "userId": "09887141178389253930"
     },
     "user_tz": -180
    },
    "id": "J96qwBOfEOk-"
   },
   "outputs": [],
   "source": [
    "def test2mask(path):\n",
    "    to_tensor_ = tvt.ToTensor()\n",
    "    im = to_tensor_(Image.open(path)).unsqueeze(0).to(dev)\n",
    "    X = tr.cat([edges_x1(im), edges_x2(im), edges_x3(im)], dim=-3)\n",
    "    pred = (best_model(X, 0) > 0.5).int() * 255\n",
    "    return pred.squeeze().cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jnQlL67VbZY"
   },
   "source": [
    "Сохраняем html c результатом на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRH6kzIrQDjR"
   },
   "outputs": [],
   "source": [
    "paths_to_imgs = sorted(glob(path0 + \"/data/test/*\"))\n",
    "pred_masks = [test2mask(path) for path in paths_to_imgs]\n",
    "_ = get_html(paths_to_imgs, pred_masks, path_to_save = path0 + \"/results/results_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что можно добавить для дальнейшего повышения качества:\n",
    "\n",
    "1. CRF которых неплохо делает постобработку.\n",
    "2. модель с заменой фона и более креативной аугментацией.\n",
    "3. ансамбли \n",
    "4. компрессию информации в маленькую якорную модель.\n",
    "5. более сложные модели на основе преобразованний Фурье с подбором спектра и полноразмерными (как фото)  управляемыми сверточными ядрами Гаусса.\n",
    "6. на Pyramid-подобные архитектуры\n",
    "7. на графовые модели с инцедентными матрицами высокоуровневых признаков.\n",
    "8. на трансформеры. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "mipt_test1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07a174144c11429d95cd976e5226f587": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ca328ccad094c9bb80611373ff49bfc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "19e60de5a7c94ef8bb7a17bd69a871bf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5335b1b1dd92421bb0c0d5c253d159dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e30ec208050146afb110b6c960a50d1c",
       "IPY_MODEL_d40156fc98e6440e990d6fbcb1f6c01e"
      ],
      "layout": "IPY_MODEL_da1307cdbdba46a1b88e8aadae0ba7ed"
     }
    },
    "8b5e34bb52114d688816b90c3270f22f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8c54da2599b84f79afad86412c3d3fd4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d27b5d5cfdfd4692bc5a9e754b4de813",
       "IPY_MODEL_d41cb2b44eb14fffa1be82a1e5d6ca80"
      ],
      "layout": "IPY_MODEL_f5ac3d6afe094fb8a417e2f396517359"
     }
    },
    "9346e7b0eff8473fa88a00ea97d6288e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "a9b822fb488446a19e43ffab8262e0ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d27b5d5cfdfd4692bc5a9e754b4de813": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_07a174144c11429d95cd976e5226f587",
      "max": 145,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9346e7b0eff8473fa88a00ea97d6288e",
      "value": 145
     }
    },
    "d40156fc98e6440e990d6fbcb1f6c01e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_19e60de5a7c94ef8bb7a17bd69a871bf",
      "placeholder": "​",
      "style": "IPY_MODEL_8b5e34bb52114d688816b90c3270f22f",
      "value": " 1315/1315 [00:08&lt;00:00, 160.03it/s]"
     }
    },
    "d41cb2b44eb14fffa1be82a1e5d6ca80": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed1f900ec99645a6add27986bdf60dab",
      "placeholder": "​",
      "style": "IPY_MODEL_a9b822fb488446a19e43ffab8262e0ee",
      "value": " 145/145 [00:05&lt;00:00, 27.94it/s]"
     }
    },
    "da1307cdbdba46a1b88e8aadae0ba7ed": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2c401fee0fa433d85578bd9694c9733": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e30ec208050146afb110b6c960a50d1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e2c401fee0fa433d85578bd9694c9733",
      "max": 1315,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0ca328ccad094c9bb80611373ff49bfc",
      "value": 1315
     }
    },
    "ed1f900ec99645a6add27986bdf60dab": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f5ac3d6afe094fb8a417e2f396517359": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
